{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91e552aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# USER-BASED PREPROCESSING + FEATURE ENGINEERING\n",
    "# ----------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43ddb44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded!\n",
      "  TransactionID AccountID  TransactionAmount      TransactionDate  \\\n",
      "0      TX000001   AC00128              14.09  2023-04-11 16:29:14   \n",
      "1      TX000002   AC00455             376.24  2023-06-27 16:44:19   \n",
      "2      TX000003   AC00019             126.29  2023-07-10 18:16:08   \n",
      "3      TX000004   AC00070             184.50  2023-05-05 16:32:11   \n",
      "4      TX000005   AC00411              13.45  2023-10-16 17:51:24   \n",
      "\n",
      "  TransactionType   Location DeviceID      IP Address MerchantID Channel  \\\n",
      "0           Debit  San Diego  D000380  162.198.218.92       M015     ATM   \n",
      "1           Debit    Houston  D000051     13.149.61.4       M052     ATM   \n",
      "2           Debit       Mesa  D000235  215.97.143.157       M009  Online   \n",
      "3           Debit    Raleigh  D000187  200.13.225.150       M002  Online   \n",
      "4          Credit    Atlanta  D000308    65.164.3.100       M091  Online   \n",
      "\n",
      "   CustomerAge CustomerOccupation  TransactionDuration  LoginAttempts  \\\n",
      "0           70             Doctor                   81              1   \n",
      "1           68             Doctor                  141              1   \n",
      "2           19            Student                   56              1   \n",
      "3           26            Student                   25              1   \n",
      "4           26            Student                  198              1   \n",
      "\n",
      "   AccountBalance PreviousTransactionDate  \n",
      "0         5112.21     2024-11-04 08:08:08  \n",
      "1        13758.91     2024-11-04 08:09:35  \n",
      "2         1122.35     2024-11-04 08:07:04  \n",
      "3         8569.06     2024-11-04 08:09:06  \n",
      "4         7429.40     2024-11-04 08:06:39  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2512 entries, 0 to 2511\n",
      "Data columns (total 16 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   TransactionID            2512 non-null   object \n",
      " 1   AccountID                2512 non-null   object \n",
      " 2   TransactionAmount        2512 non-null   float64\n",
      " 3   TransactionDate          2512 non-null   object \n",
      " 4   TransactionType          2512 non-null   object \n",
      " 5   Location                 2512 non-null   object \n",
      " 6   DeviceID                 2512 non-null   object \n",
      " 7   IP Address               2512 non-null   object \n",
      " 8   MerchantID               2512 non-null   object \n",
      " 9   Channel                  2512 non-null   object \n",
      " 10  CustomerAge              2512 non-null   int64  \n",
      " 11  CustomerOccupation       2512 non-null   object \n",
      " 12  TransactionDuration      2512 non-null   int64  \n",
      " 13  LoginAttempts            2512 non-null   int64  \n",
      " 14  AccountBalance           2512 non-null   float64\n",
      " 15  PreviousTransactionDate  2512 non-null   object \n",
      "dtypes: float64(2), int64(3), object(11)\n",
      "memory usage: 314.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 1. Load dataset\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\user\\\\Desktop\\\\Anomaly Detection\\\\Linklock\\\\bank_transactions_data_2.csv\")\n",
    "\n",
    "print(\"Dataset Loaded!\")\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ec4ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 2. Convert Date Columns\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "df['TransactionDate'] = pd.to_datetime(df['TransactionDate'], errors='coerce')\n",
    "df['PreviousTransactionDate'] = pd.to_datetime(df['PreviousTransactionDate'], errors='coerce')\n",
    "\n",
    "# Sort by date to compute transitions\n",
    "df = df.sort_values(by=['AccountID', 'TransactionDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "054cf8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 3. USER-BASED FEATURE ENGINEERING\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "user_df = pd.DataFrame()\n",
    "\n",
    "# Base User Identifier\n",
    "user_df['AccountID'] = df.groupby('AccountID').size().index\n",
    "\n",
    "# ---- LOGIN PATTERN FEATURES ----\n",
    "\n",
    "# Avg login hour\n",
    "user_df['avg_login_hour'] = df.groupby('AccountID')['TransactionDate'].apply(lambda x: x.dt.hour.mean())\n",
    "\n",
    "# Std deviation of login hour (variability)\n",
    "user_df['std_login_hour'] = df.groupby('AccountID')['TransactionDate'].apply(lambda x: x.dt.hour.std())\n",
    "\n",
    "# Day of week login pattern\n",
    "user_df['most_common_dow'] = df.groupby('AccountID')['TransactionDate'].apply(lambda x: x.dt.dayofweek.mode()[0])\n",
    "\n",
    "\n",
    "# ---- LOGIN FREQUENCY FEATURES ----\n",
    "\n",
    "# Time between two consecutive logins (seconds)\n",
    "df['time_between_logins'] = df.groupby('AccountID')['TransactionDate'].diff().dt.total_seconds()\n",
    "\n",
    "# Replace missing values with median\n",
    "df['time_between_logins'] = df['time_between_logins'].fillna(df['time_between_logins'].median())\n",
    "\n",
    "# Avg login interval\n",
    "user_df['avg_login_interval'] = df.groupby('AccountID')['time_between_logins'].mean()\n",
    "\n",
    "# Std deviation of login interval\n",
    "user_df['std_login_interval'] = df.groupby('AccountID')['time_between_logins'].std()\n",
    "\n",
    "\n",
    "# ---- DEVICE / IP / LOCATION CONSISTENCY ----\n",
    "\n",
    "# How many unique devices does a user normally use?\n",
    "user_df['unique_devices'] = df.groupby('AccountID')['DeviceID'].nunique()\n",
    "\n",
    "# Unique IP addresses\n",
    "user_df['unique_ips'] = df.groupby('AccountID')['IP Address'].nunique()\n",
    "\n",
    "# Unique locations\n",
    "user_df['unique_locations'] = df.groupby('AccountID')['Location'].nunique()\n",
    "\n",
    "# Ratio of unusual device changes\n",
    "user_df['device_change_rate'] = user_df['unique_devices'] / df.groupby('AccountID').size()\n",
    "\n",
    "# Ratio of unusual IP changes\n",
    "user_df['ip_change_rate'] = user_df['unique_ips'] / df.groupby('AccountID').size()\n",
    "\n",
    "\n",
    "# ---- FAILED LOGIN ATTEMPTS ----\n",
    "\n",
    "user_df['avg_login_attempts'] = df.groupby('AccountID')['LoginAttempts'].mean()\n",
    "user_df['max_login_attempts'] = df.groupby('AccountID')['LoginAttempts'].max()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cedc4e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------\n",
    "# 4. Handle missing values\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "user_df = user_df.fillna(user_df.median(numeric_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d63ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_encoders.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 5. Encode Categorical Columns \n",
    "# ----------------------------------------------------------\n",
    "\n",
    "encoders = {}\n",
    "\n",
    "enc = LabelEncoder()\n",
    "user_df['AccountID'] = enc.fit_transform(user_df['AccountID'])\n",
    "encoders['AccountID'] = enc\n",
    "\n",
    "# Save encoder\n",
    "joblib.dump(encoders, \"user_encoders.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03a702e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['user_scaler.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 6. Scale Numerical Features\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "scale_cols = [\n",
    "    'avg_login_hour', 'std_login_hour', 'most_common_dow',\n",
    "    'avg_login_interval', 'std_login_interval',\n",
    "    'unique_devices', 'unique_ips', 'unique_locations',\n",
    "    'device_change_rate', 'ip_change_rate',\n",
    "    'avg_login_attempts', 'max_login_attempts'\n",
    "]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "user_df[scale_cols] = scaler.fit_transform(user_df[scale_cols])\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, \"user_scaler.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91bc91d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "USER-BASED PREPROCESSING COMPLETE!\n",
      "Saved -> user_behavior_features.csv, user_scaler.pkl, user_encoders.pkl\n",
      "\n",
      "Final User Behavior Feature Matrix:\n",
      "   AccountID  avg_login_hour  std_login_hour  most_common_dow  \\\n",
      "0          0             NaN             NaN              NaN   \n",
      "1          1             NaN             NaN              NaN   \n",
      "2          2             NaN             NaN              NaN   \n",
      "3          3             NaN             NaN              NaN   \n",
      "4          4             NaN             NaN              NaN   \n",
      "\n",
      "   avg_login_interval  std_login_interval  unique_devices  unique_ips  \\\n",
      "0                 NaN                 NaN             NaN         NaN   \n",
      "1                 NaN                 NaN             NaN         NaN   \n",
      "2                 NaN                 NaN             NaN         NaN   \n",
      "3                 NaN                 NaN             NaN         NaN   \n",
      "4                 NaN                 NaN             NaN         NaN   \n",
      "\n",
      "   unique_locations  device_change_rate  ip_change_rate  avg_login_attempts  \\\n",
      "0               NaN                 NaN             NaN                 NaN   \n",
      "1               NaN                 NaN             NaN                 NaN   \n",
      "2               NaN                 NaN             NaN                 NaN   \n",
      "3               NaN                 NaN             NaN                 NaN   \n",
      "4               NaN                 NaN             NaN                 NaN   \n",
      "\n",
      "   max_login_attempts  \n",
      "0                 NaN  \n",
      "1                 NaN  \n",
      "2                 NaN  \n",
      "3                 NaN  \n",
      "4                 NaN  \n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 7. Save final user feature matrix\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "user_df.to_csv(\"user_behavior_features.csv\", index=False)\n",
    "\n",
    "print(\"\\nUSER-BASED PREPROCESSING COMPLETE!\")\n",
    "print(\"Saved -> user_behavior_features.csv, user_scaler.pkl, user_encoders.pkl\")\n",
    "print(\"\\nFinal User Behavior Feature Matrix:\")\n",
    "print(user_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
